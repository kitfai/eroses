import boto3
import os
from datetime import datetime
from sqlalchemy import create_engine, Column, Integer, String, Float, DateTime, Boolean
from sqlalchemy.ext.declarative import declarative_base
from sqlalchemy.orm import sessionmaker
from fuzzywuzzy import fuzz
from fuzzywuzzy import process
import pandas as pd
import numpy as np
from typing import List, Dict, Tuple
import logging
from botocore.config import Config

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)
import urllib.parse
import time

from typing import List, Dict
from botocore.exceptions import ClientError
import json
# SQLAlchemy setup
#Base = declarative_base()



# SQLAlchemy setup
Base = declarative_base()




class S3DirectoryNavigator:
    def __init__(self, bucket_name: str):
        self.s3_client = boto3.client('s3')
        self.bucket_name = bucket_name

    def list_directories(self, prefix: str) -> List[str]:
        """List all directories under the given prefix"""
        try:
            paginator = self.s3_client.get_paginator('list_objects_v2')
            directories = set()

            for page in paginator.paginate(Bucket=self.bucket_name, Prefix=prefix, Delimiter='/'):
                if 'CommonPrefixes' in page:
                    for prefix_obj in page['CommonPrefixes']:
                        dir_name = prefix_obj['Prefix'].rstrip('/').split('/')[-1]
                        directories.add(dir_name)

            return sorted(list(directories))
        except ClientError as e:
            print(f"Error listing directories: {e}")
            return []

    def list_pdfs(self, prefix: str) -> List[str]:
        """List all PDF files in the given prefix"""
        try:
            paginator = self.s3_client.get_paginator('list_objects_v2')
            pdf_files = []

            for page in paginator.paginate(Bucket=self.bucket_name, Prefix=prefix):
                if 'Contents' in page:
                    for obj in page['Contents']:
                        if obj['Key'].lower().endswith('.pdf'):
                            pdf_name = obj['Key'].split('/')[-1]
                            pdf_files.append(pdf_name)

            return sorted(pdf_files)
        except ClientError as e:
            print(f"Error listing PDFs: {e}")
            return []


class PartyDirectoryProcessor:
    def __init__(self, bucket_name: str, root_directory: str, db_connection_string: str):
        self.navigator = S3DirectoryNavigator(bucket_name)
        self.root_directory = root_directory.rstrip('/')
        self.engine = create_engine(db_connection_string)
        Base.metadata.create_all(self.engine)
        Session = sessionmaker(bind=self.engine)
        self.session = Session()
        self.textract_client = boto3.client('textract', region_name='ap-southeast-1')
        self.bucket_name = bucket_name

        self.standard_columns = {
            "JAWATAN": ["jawatan", "position", "pangkat"],
            "NAMA": ["nama", "name", "nama penuh"],
            "NO_KP": ["no. kad pengenalan", "no kp", "no. kp", "no. ic", "no ic"],
            "TELEFON": ["no telefon", "no. tel", "tel.", "telefon", "phone"],
            "ALAMAT": ["alamat", "address", "tempat tinggal"],
            "MAJIKAN": ["majikan", "nama majikan", "tempat kerja","perkerjaan, nama dan alamat majikan", "alamat majikan"],
            "TAHUN": ["tahun", "year", "tahun berkuatkuasa"],
            "JANTINA": ["jantina", "gender", "sex"]
        }

        # Configuration parameters
        self.min_match_score = 75
        self.confidence_threshold = 90.0
        self.max_line_length = 200

        # Processing state
        self.current_file_id = None
        self.current_page = None
        self.current_table = None

    def process_perlembagaan(self, party_dir: str) -> Dict:
        """Process Perlembagaan directory for a party"""
        base_path = f"{self.root_directory}/{party_dir}/Induk/Perlembagaan"
        result = {
            'directory_name': 'Perlembagaan',
            'subdirectories': {}
        }

        # List directories in Perlembagaan
        directories = self.navigator.list_directories(f"{base_path}/")

        # Process each directory
        for directory in directories:
            pdfs = self.navigator.list_pdfs(f"{base_path}/{directory}/")
            result['subdirectories'][directory] = pdfs

        return result

    def process_sijil_specific_party(self, party_dir: str):
        """Process Sijil directory for a party"""
        # base_path = f"{self.root_directory}/{party_dir}/Induk/SIJIL" #to use this
        base_path = f"{self.root_directory}/{party_dir}/Induk/Senarai AJK"
        result = {
            'directory_name': 'Sijil',
            'subdirectories': {}
        }

        self.process_years(base_path)  # process year

    def process_years(self, base_path: str):
        directories = self.navigator.list_directories(f"{base_path}/")  # ['years']

        # Process each directory
        for year in directories:  # process every year
            self.process_sijil_dir(base_path, year)  # check for bendera directory

    def process_sijil_dir(self, base_path: str, year: str):
        #directories = self.navigator.list_directories(f"{base_path}/{year}/")  # ['years']

        pdfs = self.navigator.list_pdfs(f"{base_path}/{year}/")

        for pdf in pdfs:
            print(f"PDF: {pdf}")
            print(f"Path: {base_path}/{year}/{pdf}")
            self.process_ajk_pdf(f"{base_path}/{year}/{pdf}", base_path, year, pdf)

    def process_ajk_dir(self, base_path: str, year: str, sijil_name: str):

        pdfs = self.navigator.list_pdfs(f"{base_path}/{year}/{sijil_name}/")

        for pdf in pdfs:
            print(f"PDF: {pdf}")
            print(f"Path: {base_path}/{year}/{sijil_name}/{pdf}")
            self.process_ajk_pdf(f"{base_path}/{year}/{sijil_name}/{pdf}", base_path, year, pdf)

    def process_ajk_pdf(self, path: str, base_path: str, year: str, pdf: str):
        """Process a single PDF file from S3 with enhanced error handling"""
        s3_path = f'{base_path}/{year}/{pdf}'
        processed_file = None
        try:
            # Check if file was already processed
            '''existing_file = self.session.query(ProcessedFile).filter_by(
                s3_path=s3_path
            ).first()

            if existing_file and existing_file.is_processed:
                logger.info(f"Skipping already processed file: {s3_path}")
                return'''

            # Start Textract job
            response = self.textract_client.start_document_analysis(
                DocumentLocation={
                    'S3Object': {
                        'Bucket': self.bucket_name,
                        'Name': s3_path
                    }
                },
                FeatureTypes=['TABLES']
            )

            job_id = response['JobId']
            logger.info(f"Started Textract job {job_id} for {s3_path}")

            # Create or update processed file record
            '''processed_file = existing_file or ProcessedFile(
                s3_path=s3_path,
                filename=os.path.basename(s3_path),
                year_directory=year_dir
            )

            if not existing_file:
                self.session.add(processed_file)
                self.session.flush()'''

            #self.current_file_id = processed_file.id

            # Get all pages data
            pages_data = self.wait_and_get_results(job_id)
            #processed_file.total_pages = len(pages_data)
            processed_file = {
                'last_processed_page': 0,
                'processed_pages': 0,
                'is_processed': False,
                'error_message': None
            }
            # Process each page
            for page_num, page_data in enumerate(pages_data, 1):
                #if page_num <= processed_file.last_processed_page:
                #    logger.info(f"Skipping already processed page {page_num}")
                #    continue

                try:
                    self.current_page = page_num
                    tables = self.extract_table_data(page_data['Blocks'], page_num)

                    for table_num, table in enumerate(tables, 1):
                        self.current_table = table_num
                        self.store_extracted_data(page_num, table)

                    processed_file.last_processed_page = page_num
                    processed_file.processed_pages += 1
                    self.session.commit()
                    logger.info(f"Processed page {page_num} of {len(pages_data)}")

                except Exception as e:
                    logger.error(f"Error processing page {page_num}: {str(e)}")
                    continue

            processed_file.is_processed = True
            #self.session.commit()
            logger.info(f"Successfully processed {s3_path}")

        except Exception as e:
            logger.error(f"Error processing {s3_path}: {str(e)}")
            '''if processed_file:
                processed_file.error_message = str(e)
                self.session.commit()
            self.session.rollback()'''
            raise

    def extract_table_data(self, blocks: List[Dict], page_num: int) -> List[Dict]:
        """Extract and process table data from Textract blocks"""
        tables = []
        current_table = None
        valid_columns = {}
        table_count = 0
        unprocessed_batch = []

        for block in blocks:
            if block['BlockType'] == 'TABLE':
                table_count += 1
                if current_table and valid_columns:
                    tables.append(current_table)
                current_table = {
                    'table_number': table_count,
                    'cells': {},
                    'column_headers': {},
                    'num_rows': block.get('RowCount', 0),
                    'num_cols': block.get('ColumnCount', 0)
                }
                valid_columns = {}

            elif block['BlockType'] == 'CELL' and current_table is not None:
                row_index = block.get('RowIndex', 0)
                col_index = block.get('ColumnIndex', 0)

                # Extract raw content and confidence
                raw_content = self.extract_cell_text(block, blocks)
                confidence = block.get('Confidence', 0.0)

                # Clean the content
                cleaned_content, num_lines = self.clean_cell_content(raw_content)

                # Process header row (row 1) - Don't store headers in UnprocessedCells
                if row_index == 1:
                    std_name, match_score = self.standardize_column_name(cleaned_content)
                    if std_name:
                        valid_columns[col_index] = {
                            'name': std_name,
                            'match_score': match_score,
                            'original': cleaned_content
                        }
                        current_table['column_headers'][col_index] = valid_columns[col_index]
                    continue  # Skip storing headers in unprocessed cells

                # Process data cells (rows 2 and beyond)
                if row_index > 1:
                    # For cells in columns that don't match standard headers
                    if col_index not in valid_columns:
                        unprocessed_batch.append({
                            'page_num': page_num,
                            'table_num': table_count,
                            'row_num': row_index,
                            'column_name': f"Column_{col_index}",
                            'content': raw_content,  # Store the cell value
                            'confidence': confidence,
                            'reason': "Column not in standard columns"
                        })
                        continue

                    # For cells with low confidence scores
                    if confidence < self.confidence_threshold:
                        unprocessed_batch.append({
                            'page_num': page_num,
                            'table_num': table_count,
                            'row_num': row_index,
                            'column_name': valid_columns[col_index]['name'],
                            'content': raw_content,  # Store the cell value
                            'confidence': confidence,
                            'reason': "Low confidence score"
                        })
                        continue

                    # For empty or invalid content
                    if not cleaned_content:
                        unprocessed_batch.append({
                            'page_num': page_num,
                            'table_num': table_count,
                            'row_num': row_index,
                            'column_name': valid_columns[col_index]['name'],
                            'content': raw_content,  # Store the cell value
                            'confidence': confidence,
                            'reason': "Empty or invalid content"
                        })
                        continue

                    # Store valid cell data
                    current_table['cells'][(row_index, col_index)] = {
                        'content': cleaned_content,
                        'confidence': confidence,
                        'num_lines': num_lines,
                        'column_name': valid_columns[col_index]['name']
                    }

                # Process unprocessed cells in batches
                if len(unprocessed_batch) >= 100:
                    self.store_unprocessed_cells_batch(unprocessed_batch)
                    unprocessed_batch = []

        # Process any remaining unprocessed cells
        if unprocessed_batch:
            self.store_unprocessed_cells_batch(unprocessed_batch)

        if current_table and valid_columns:
            tables.append(current_table)

        return tables

    def standardize_column_name(self, original_name: str) -> Tuple[str, float]:
        """Find the closest matching standard column name with enhanced matching"""
        if not original_name:
            return "", 0.0

        cleaned_name = ' '.join(original_name.strip().split()).lower()
        best_match = ("", 0.0)

        # Try exact match first
        for standard_name, variations in self.standard_columns.items():
            if cleaned_name in variations or cleaned_name == standard_name.lower():
                return standard_name, 100.0

        # If no exact match, try fuzzy matching
        for standard_name, variations in self.standard_columns.items():
            # Check against standard name
            score = fuzz.token_sort_ratio(cleaned_name, standard_name.lower())
            if score > best_match[1] and score >= self.min_match_score:
                best_match = (standard_name, score)

            # Check against variations
            for variation in variations:
                score = fuzz.token_sort_ratio(cleaned_name, variation)
                if score > best_match[1] and score >= self.min_match_score:
                    best_match = (standard_name, score)

        return best_match

    def is_signature_line(self, text: str) -> bool:
        """Check if text is part of signature section"""
        signature_indicators = [
            'tandatangan', 'signature', 't/tangan', 't.t', 't/t',
            'cop', 'stamp', 'seal', 'meterai', 'disahkan',
            'pengesahan', 'certified', 'verified', 'endorsed'
        ]
        text_lower = text.lower()
        return any(indicator in text_lower for indicator in signature_indicators)

    def clean_cell_content(self, content: str) -> Tuple[str, int]:
        """Clean cell content and handle newlines"""
        if not content:
            return "", 0

        lines = [line.strip() for line in content.split('\n')]
        cleaned_lines = []

        for line in lines:
            if line and not self.is_signature_line(line):
                # Remove multiple spaces
                line = ' '.join(line.split())
                if len(line) <= self.max_line_length:
                    cleaned_lines.append(line)

        if not cleaned_lines:
            return "", 0

        return '\n'.join(cleaned_lines), len(cleaned_lines)
    def extract_cell_text(self, cell_block: Dict, blocks: List[Dict]) -> str:
        """Extract text from cell block"""
        text_parts = []

        if 'Relationships' in cell_block:
            for rel in cell_block['Relationships']:
                if rel['Type'] == 'CHILD':
                    for word_id in rel['Ids']:
                        word_block = next((b for b in blocks if b['Id'] == word_id), None)
                        if word_block and word_block['BlockType'] == 'WORD':
                            text_parts.append(word_block['Text'])

        return ' '.join(text_parts)

    def store_extracted_data(self, page_num: int, table_data: Dict):
        """Store extracted table data in the database"""
        try:
            key_value = {}
            if not table_data.get('cells'):
                logger.warning(f"No cells found in table on page {page_num}")
                return

            stored_count = 0
            for (row_idx, col_idx), cell_data in table_data['cells'].items():
                if col_idx not in table_data['column_headers']:
                    continue

                column_info = table_data['column_headers'][col_idx]

                # Skip if content is empty
                if not cell_data.get('content'):
                    continue

                key_value[column_info['name']] = cell_data['content']

                '''extracted_data = ExtractedTableData(
                    file_id=self.current_file_id,
                    page_number=page_num,
                    table_number=table_data['table_number'],
                    row_number=row_idx,
                    column_name=column_info['name'],  # Changed from 'standardized'
                    content=cell_data['content'],
                    confidence_score=cell_data['confidence']
                )

                self.session.add(extracted_data)'''
                stored_count += 1

                # Commit in batches to improve performance
                '''if stored_count % 100 == 0:
                    self.session.flush()
            
            if stored_count > 0:
                self.session.commit()
                logger.info(f"Stored {stored_count} rows for page {page_num}, table {table_data['table_number']}")
            else:
                logger.warning(f"No valid data to store for page {page_num}, table {table_data['table_number']}")'''

            logger.info('hey')

        except Exception as e:
            logger.error(f"Error storing extracted data: {str(e)}")
            #self.session.rollback()
            raise

    def store_unprocessed_cells_batch(self, unprocessed_batch: List[Dict]):
        """Store a batch of unprocessed cells with their values"""
        return
        try:
            unprocessed_records = []
            for cell in unprocessed_batch:
                unprocessed = UnprocessedCells(
                    file_id=self.current_file_id,
                    page_number=cell['page_num'],
                    table_number=cell['table_num'],
                    row_number=cell['row_num'],
                    column_name=cell['column_name'],
                    content=cell['content'],  # Store the actual cell value
                    confidence_score=cell['confidence'],
                    reason=cell['reason']
                )
                unprocessed_records.append(unprocessed)

            # Bulk insert the records
            try:
                self.session.bulk_save_objects(unprocessed_records)
                self.session.flush()
                logger.debug(f"Stored {len(unprocessed_records)} unprocessed cells in batch")
            except Exception as flush_error:
                logger.error(f"Error flushing unprocessed cells batch: {str(flush_error)}")
                self.session.rollback()
                raise

        except Exception as e:
            logger.error(f"Error storing unprocessed cells batch: {str(e)}")
            self.session.rollback()
            raise

    def wait_and_get_results(self, job_id: str) -> List[Dict]:
        """Wait for job completion and get all results"""
        # Wait for job completion
        while True:
            response = self.textract_client.get_document_analysis(JobId=job_id)
            status = response['JobStatus']

            if status == 'SUCCEEDED':
                break
            elif status == 'FAILED':
                raise Exception(f"Textract job failed: {response.get('StatusMessage')}")
            elif status in ['IN_PROGRESS', 'SUBMITTED']:
                time.sleep(5)
                continue
            else:
                raise Exception(f"Unknown job status: {status}")

        # Get all results
        pages_data = []
        next_token = None

        while True:
            try:
                if next_token:
                    response = self.textract_client.get_document_analysis(
                        JobId=job_id,
                        NextToken=next_token,
                        MaxResults=1000
                    )
                else:
                    response = self.textract_client.get_document_analysis(
                        JobId=job_id,
                        MaxResults=1000
                    )

                if 'Blocks' in response:
                    pages_data.append(response)

                next_token = response.get('NextToken')
                if not next_token:
                    break

                time.sleep(0.25)  # Avoid throttling

            except Exception as e:
                logger.error(f"Error getting results for job {job_id}: {str(e)}")
                raise

        return pages_data


    def process_sijil(self, party_dir: str) -> Dict:
        """Process Sijil directory for a party"""
        base_path = f"{self.root_directory}/{party_dir}/Induk/Sijil"
        result = {
            'directory_name': 'Sijil',
            'subdirectories': {}
        }

        # List directories in Sijil
        directories = self.navigator.list_directories(f"{base_path}/")

        # Process each directory
        for directory in directories:
            pdfs = self.navigator.list_pdfs(f"{base_path}/{directory}/")
            result['subdirectories'][directory] = pdfs

        return result

    def process_ajk(self, party_dir: str) -> Dict:
        """Process AJK directory for a party"""
        base_path = f"{self.root_directory}/{party_dir}/Induk/AJK"
        result = {
            'directory_name': 'AJK',
            'subdirectories': {}
        }

        # List directories in AJK
        directories = self.navigator.list_directories(f"{base_path}/")

        # Process each directory
        for directory in directories:
            pdfs = self.navigator.list_pdfs(f"{base_path}/{directory}/")
            result['subdirectories'][directory] = pdfs

        return result

    def process_all_parties(self) -> Dict:
        """Process all party directories"""
        result = {}

        try:
            # List all party directories
            party_directories = self.navigator.list_directories(f"{self.root_directory}/")

            # Process each party directory
            for party_dir in party_directories:
                party_result = {
                    'party_name': party_dir,
                    'perlembagaan': self.process_perlembagaan(party_dir),
                    'sijil': self.process_sijil(party_dir),
                    'ajk': self.process_ajk(party_dir)
                }
                result[party_dir] = party_result

            return result
        except Exception as e:
            print(f"Error processing parties: {e}")
            return {}


def main():
    db_connection = "mysql+mysqlconnector://root:strong_password@127.0.0.1:3307/eroses_dev"
    '''bucket_name = 'digitization-migration'
    root_directory = 'Parti_Politik-Induk'''
    bucket_name = 'induk-account-training'
    root_directory = 'Parti_Politik-Induk'
    processor = PartyDirectoryProcessor(bucket_name, root_directory, db_connection)
    # result = processor.process_all_parties()
    # print(result)
    processor.process_sijil_specific_party("AMANAH")

    # Save result to JSON file
    import json


if __name__ == "__main__":
    main()
